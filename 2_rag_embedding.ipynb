{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![MGB Logo](./images/mgb-logo.png)\n",
   "id": "ff359a37580481fa"
  },
  {
   "cell_type": "code",
   "id": "3cfd9068866dbacf",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1. Importing Required Libraries\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates the necessary imports for setting up Azure OpenAI authentication, embeddings, and model interaction.\n",
    "\n",
    "# Key Components:\n",
    "#   - os: Provides functions for interacting with the operating system, such as accessing environment variables.\n",
    "#   - load_dotenv(): Loads environment variables from a .env file to securely store sensitive credentials.\n",
    "#   - DefaultAzureCredential: Handles authentication with Azure services using various credential methods.\n",
    "#   - get_bearer_token_provider(): Retrieves an authentication token for accessing Azure OpenAI services.\n",
    "#   - AzureOpenAIEmbeddings: Enables text embedding generation using Azure OpenAI.\n",
    "#   - AzureChatOpenAI: A LangChain wrapper for interacting with Azure-hosted OpenAI chat models.\n",
    "\n",
    "# Purpose:\n",
    "# These libraries facilitate secure authentication and seamless interaction with Azure OpenAI,\n",
    "# enabling applications to leverage AI-powered text embeddings and chat models for various use cases.\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from langchain_openai import AzureOpenAIEmbeddings,AzureChatOpenAI\n",
    "\n",
    "print(\"\\n=== Required Libraries Loaded ===\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2. Setting Up Azure OpenAI Embeddings and Chat Model\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to configure and authenticate Azure OpenAI embeddings and chat models for AI-powered applications.\n",
    "\n",
    "# Key Components:\n",
    "#   - load_dotenv(): Loads environment variables from a .env file to securely store API credentials.\n",
    "#   - DefaultAzureCredential(): Handles authentication with Azure services using the best available method.\n",
    "#   - get_bearer_token_provider(): Retrieves an authentication token for accessing Azure OpenAI services.\n",
    "#   - AzureOpenAIEmbeddings(): Initializes the Azure-hosted embeddings model for text vectorization.\n",
    "#       - model: Specifies the embedding model to use.\n",
    "#       - azure_deployment: Identifies the specific Azure deployment for embeddings.\n",
    "#       - api_version: Defines the API version to be used.\n",
    "#       - azure_endpoint: Specifies the Azure endpoint for API access.\n",
    "#       - azure_ad_token_provider: Supplies authentication tokens.\n",
    "#       - timeout: Ensures requests never time out (None).\n",
    "#       - max_retries: Sets the number of retries in case of failure (2).\n",
    "#   - AzureChatOpenAI(): Initializes the Azure-hosted OpenAI chat model.\n",
    "#       - openai_api_version: Specifies the API version.\n",
    "#       - azure_deployment: Identifies the specific chat model deployment.\n",
    "#       - azure_endpoint: Defines the endpoint for API access.\n",
    "#       - azure_ad_token_provider: Supplies authentication tokens.\n",
    "\n",
    "# Purpose:\n",
    "# This setup ensures secure and efficient access to Azure OpenAI services for both text embeddings and conversational AI.\n",
    "# It enables scalable and robust AI-powered applications by integrating vector-based retrieval and LLM interactions.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Azure credentials and token provider\n",
    "azure_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(\n",
    "    azure_credential, \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# Initialize the AzureOpenAIEmbeddings model using environment variables\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"AZURE_EMBEDDING_MODEL\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_EMBEDDING_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    timeout=None,  # never timeout\n",
    "    max_retries=2,  # try again twice\n",
    ")\n",
    "\n",
    "# Initialize the AzureChatOpenAI model using environment variables\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_ad_token_provider=token_provider\n",
    ")\n",
    "\n",
    "print(\"\\n=== LLM and Embedding Models Loaded ===\")\n"
   ],
   "id": "3b21399788e8a846",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Option 1: Reading, Appending, Splitting Notes in Chunks\n",
    "\n",
    "This approach processes clinical notes by reading them from files, storing them in memory, and then splitting them into smaller chunks for optimized retrieval.\n",
    "\n",
    "**Key Steps:**\n",
    " 1. **Reading and Appending Clinical Notes (Item 3.1)**:\n",
    "    - Loads clinical notes from a directory and stores them in a list.\n",
    "    - Extracts metadata, such as the patient identifier, for later reference.\n",
    "\n",
    " 2. **Splitting Clinical Notes into Chunks (Item 3.2)**:\n",
    "    - Uses a text splitter to divide documents into smaller sections.\n",
    "    - Maintains context overlap between chunks to ensure coherence.\n",
    "\n",
    " 3. **Storing Chunked Clinical Notes in ChromaDB (Item 3.3)**:\n",
    "    - Converts text chunks into embeddings and stores them in ChromaDB.\n",
    "    - Enables efficient vector search for fast and relevant retrieval.\n",
    "\n",
    "**Purpose:**\n",
    "This method enhances document retrieval accuracy by breaking down large patient notes into smaller, contextually rich segments that can be efficiently searched and analyzed.\n",
    "\n",
    "![RAG Chunks](images/rag_chunks.png)"
   ],
   "id": "ec54fa48866bf85e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.1. Reading and Appending Clinical Notes (OPTION 1)\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to read and store clinical notes from a directory for further processing.\n",
    "\n",
    "# Key Components:\n",
    "#   - clinical_notes_dir: Specifies the directory where patient notes are stored.\n",
    "#   - documents: A list that stores the text content of each clinical note.\n",
    "#   - metadata: A list that stores metadata (patient identifiers) associated with each note.\n",
    "#   - sorted(os.listdir(clinical_notes_dir)): Retrieves and sorts all filenames in the directory.\n",
    "#   - filename.endswith(\".txt\"): Ensures only text files are processed.\n",
    "#   - patient_num: Extracts the patient identifier from the filename.\n",
    "#   - open(file_path, \"r\", encoding=\"utf-8\"): Reads the file contents while preserving character encoding.\n",
    "#   - documents.append(text): Stores the full text of each clinical note.\n",
    "#   - metadata.append({\"patient_num\": patient_num}): Associates each note with its respective patient ID.\n",
    "\n",
    "# Purpose:\n",
    "# This step prepares clinical notes for further processing, such as embedding for retrieval-augmented generation (RAG).\n",
    "# It ensures that patient records are correctly loaded and structured before being indexed in a vector store.\n",
    "\n",
    "clinical_notes_dir = 'data_prep/patient_notes'\n",
    "\n",
    "# Prepare documents list\n",
    "documents = []\n",
    "metadata = []\n",
    "\n",
    "# Read and process each clinical note\n",
    "for filename in sorted(os.listdir(clinical_notes_dir)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Extract patient identifier\n",
    "        parts = filename.split(\"_\")\n",
    "        patient_num = parts[1]\n",
    "        \n",
    "        # Extract visit date\n",
    "        latest_fact = parts[-1].replace(\".txt\", \"\")\n",
    "        visit_date = datetime.strptime(latest_fact, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "        \n",
    "        # Load text content\n",
    "        file_path = os.path.join(clinical_notes_dir, filename)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Store document with metadata\n",
    "        documents.append(text)\n",
    "        metadata.append({\"patient_num\": patient_num, \"visit_date\": visit_date})\n",
    "\n",
    "print(documents[:50])\n"
   ],
   "id": "396965a6b29541ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.2. Splitting Clinical Notes into Chunks\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to split clinical notes into manageable chunks to optimize retrieval performance.\n",
    "\n",
    "# Key Components:\n",
    "#   - RecursiveCharacterTextSplitter: A text-splitting utility that ensures chunks are broken at logical points.\n",
    "#   - chunk_size=1000: Specifies the maximum size (in characters) of each text chunk.\n",
    "#   - chunk_overlap=50: Ensures a 50-character overlap between consecutive chunks to maintain context continuity.\n",
    "#   - text_splitter.create_documents(documents, metadatas=metadata): Splits the documents while preserving metadata.\n",
    "#   - print(split_docs[:500]): Displays the first 500 characters of the split documents for verification.\n",
    "\n",
    "# Purpose:\n",
    "# This step enhances document retrieval by ensuring that AI models process information in smaller, contextually rich segments.\n",
    "# It improves search accuracy and relevance in retrieval-augmented generation (RAG) applications.\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=350)\n",
    "split_docs = text_splitter.create_documents(documents, metadatas=metadata)\n",
    "\n",
    "# print(split_docs[:500])\n",
    "\n",
    "# Print structured chunked documents with excerpts\n",
    "print(\"\\n=== Split Document Excerpts ===\")\n",
    "for idx, doc in enumerate(split_docs[:5], 1):  \n",
    "    print(f\"Chunk {idx}:\")\n",
    "    print(f\"  Patient Num: {doc.metadata.get('patient_num', 'N/A')}\")\n",
    "    print(f\"  Visit Date: {doc.metadata.get('visit_date', 'N/A')}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:2000]}...\")  \n",
    "    print(\"-\" * 100)  \n"
   ],
   "id": "a6a41dd4dfdfbcbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.3. Storing Chunked Clinical Notes in ChromaDB\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to store and index chunked clinical notes in ChromaDB for efficient retrieval.\n",
    "\n",
    "# Key Components:\n",
    "#   - Chroma: A vector database optimized for storing and retrieving text embeddings.\n",
    "#   - Chroma.from_documents(): Creates a vector store from the split clinical notes.\n",
    "#       - split_docs: The chunked documents containing patient notes.\n",
    "#       - embedding_model: The embedding model used to convert text into vector representations.\n",
    "#       - persist_directory=\"./chroma_db_chunks\": Specifies the storage directory for the vector database.\n",
    "#   - print(\"Vector store created and loaded successfully!\"): Confirms successful creation and storage of embeddings.\n",
    "\n",
    "# Purpose:\n",
    "# This step enables fast and accurate retrieval of clinical information by storing vector embeddings of chunked text.\n",
    "# It allows AI-powered applications to efficiently search and retrieve relevant medical notes based on similarity.\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store_chunks = Chroma.from_documents(split_docs, embedding_model, persist_directory=\"./databases/chroma_db_chunks\")\n",
    "\n",
    "print(\"Vector store created and loaded successfully!\")\n",
    "\n"
   ],
   "id": "1cd85d8afebbe8fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Option 2: Storing Entire Clinical Notes in ChromaDB\n",
    "This approach processes clinical notes by reading them from files and storing them **as whole documents**  in ChromaDB, preserving full patient context.\n",
    "\n",
    "**Key Steps:(Item 4.1)**\n",
    "1. **Reading and Extracting Metadata **:\n",
    "   - Loads each clinical note from a directory.\n",
    "   - Extracts the patient identifier and visit date for metadata tracking.\n",
    "\n",
    "2. **Embedding and Storing Full Documents**:\n",
    "   - Converts the entire text of each clinical note into vector embeddings.\n",
    "   - Stores them in ChromaDB, maintaining patient-level document integrity.\n",
    "\n",
    "**Purpose:**\n",
    "This method ensures that full patient records remain intact, allowing retrieval of complete medical histories instead of fragmented sections. It is ideal for cases where full context is necessary for decision-making.\n",
    "\n",
    "<img src=\"./images/rag_full.png\" alt=\"RAG Full\" width=\"900\">\n",
    "\n"
   ],
   "id": "82a748b8295bdf89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4.1. Storing Entire Clinical Notes in ChromaDB\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to store full clinical notes in ChromaDB, preserving complete patient context for retrieval.\n",
    "\n",
    "# Key Components:\n",
    "#   - Chroma: A vector database for efficiently storing and retrieving text embeddings.\n",
    "#   - Chroma(persist_directory=\"./chroma_db_full\", embedding_function=embedding_model): \n",
    "#     Initializes the vector store for storing full clinical notes.\n",
    "#   - sorted(os.listdir(clinical_notes_dir)): Retrieves and sorts all patient note files for processing.\n",
    "#   - patient_num: Extracts the patient identifier from the filename.\n",
    "#   - open(file_path, \"r\", encoding=\"utf-8\"): Reads the full content of each clinical note.\n",
    "#   - vector_store.add_texts(): Embeds and inserts the entire document into ChromaDB, associating it with metadata (patient_num).\n",
    "#   - print(\"All clinical notes have been embedded and stored successfully!\"): Confirms successful indexing of documents.\n",
    "\n",
    "# Purpose:\n",
    "# This method preserves the full clinical context of each note, allowing AI models to retrieve and analyze complete patient records.\n",
    "# It enhances retrieval accuracy by avoiding text chunking, ensuring that related medical details remain together.\n",
    "\n",
    "\n",
    "# Initialize ChromaDB vector store\n",
    "vector_store_full = Chroma(persist_directory=\"./databases/chroma_db_full\", embedding_function=embedding_model)\n",
    "\n",
    "# Process and embed each clinical note individually\n",
    "for filename in sorted(os.listdir(clinical_notes_dir)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Extract patient identifier\n",
    "        parts = filename.split(\"_\")\n",
    "        patient_num = parts[1]\n",
    "        \n",
    "        # Extract visit date\n",
    "        latest_fact = parts[-1].replace(\".txt\", \"\")\n",
    "        visit_date = datetime.strptime(latest_fact, '%Y%m%d').strftime('%m/%d/%Y')\n",
    "        \n",
    "        file_path = os.path.join(clinical_notes_dir, filename)\n",
    "\n",
    "        # Load text content\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Embed and insert document into ChromaDB immediately\n",
    "        vector_store_full.add_texts([text], metadatas=[{\"patient_num\": patient_num, \"visit_date\": visit_date}])\n",
    "\n",
    "print(\"All clinical notes have been embedded and stored successfully!\")\n"
   ],
   "id": "546719427d900c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Defining the Query for Clinical Note Retrieval\n",
   "id": "bdb9d13f6864b6ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 5. Defining the Query for Clinical Note Retrieval\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to define a natural language query for retrieving relevant clinical notes.\n",
    "\n",
    "# Key Components:\n",
    "#   - query: A user-defined question that will be used to search the vector store.\n",
    "#   - \"Who has asthma and using Fluticasone?\": The query aims to retrieve clinical notes of patients diagnosed with asthma and prescribed Fluticasone.\n",
    "\n",
    "# Purpose:\n",
    "# This query enables the retrieval of relevant patient records from the vector store, \n",
    "# allowing AI-powered applications to find medical cases that match specific conditions and medications.\n",
    "\n",
    "\n",
    "query = \"Who has asthma and is taking Fluticasone?\"\n",
    "\n",
    "\n"
   ],
   "id": "78475cf1ab2b48fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Retrieving Clinical Notes with Similarity and MMR Search\n",
    "This approach focuses on retrieving relevant clinical notes using different search methods, including similarity-based retrieval and Maximal Marginal Relevance (MMR).\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Similarity Search (Item 6.1)**:\n",
    "   - Finds the most relevant clinical notes based on vector similarity.\n",
    "   - Retrieves documents that closely match the given query.\n",
    "\n",
    "2. **Similarity Search with Relevance Scores (Item 6.2)**:\n",
    "   - Retrieves relevant documents along with their similarity scores.\n",
    "   - Enables ranking and filtering based on the confidence of relevance.\n",
    "\n",
    "3. **Using a Retriever with Score Threshold (Item 6.3)**:\n",
    "   - Configures a retriever that automatically filters documents based on a minimum similarity score.\n",
    "   - Returns only the most relevant clinical notes.\n",
    "\n",
    "4. **Maximal Marginal Relevance (MMR) Search (Item 6.4)**:\n",
    "   - Balances relevance and diversity in search results.\n",
    "   - Ensures retrieval of a broad yet relevant set of documents to avoid redundancy.\n",
    "\n",
    "**Purpose:**\n",
    "This retrieval strategy ensures that clinical notes are not only highly relevant to the query but also diverse enough to provide a well-rounded perspective. It improves search accuracy and enhances AI-driven medical analysis.\n",
    "\n",
    "<img src=\"./images/rag_retrieval.png\" alt=\"RAG Retrieval\" width=\"800\">\n",
    "\n"
   ],
   "id": "ad60855069e4016c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6.1. Performing Similarity Search\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to retrieve clinical notes based on vector similarity using cosine similarity.\n",
    "\n",
    "# Key Components:\n",
    "#   - similarity_search(query, k=10): \n",
    "#     Retrieves the top k (10 in this case) most similar documents to the query.\n",
    "#   - vector_store_chunks.similarity_search(): \n",
    "#     Searches in the chunked vector store, retrieving smaller document segments.\n",
    "#   - vector_store.similarity_search(): \n",
    "#     (Alternative) Searches in the full document vector store for broader context.\n",
    "#   - print(results): Displays the retrieved documents.\n",
    "\n",
    "# Purpose:\n",
    "# This method enables retrieval of the most relevant clinical notes based on semantic similarity,\n",
    "# allowing AI models to analyze and process medical cases that closely match the query.\n",
    "\n",
    "results = vector_store_chunks.similarity_search(query, k=10)\n",
    "# results = vector_store_full.similarity_search(query, k=5)\n",
    "# print(results)\n",
    "\n",
    "\n",
    "print(\"\\n=== Retrieved Clinical Notes ===\\n\")\n",
    "\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    print(f\"Document {idx}:\")\n",
    "    print(f\"  Patient Num: {doc.metadata.get('patient_num', 'N/A')}\")\n",
    "    print(f\"  Visit Date: {doc.metadata.get('visit_date', 'N/A')}\")\n",
    "    print(f\"  Document ID: {doc.id}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:500]}...\")  \n",
    "    print(\"-\" * 100) \n"
   ],
   "id": "25f02670f578f568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6.2. Performing Similarity Search with Relevance Scores\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to retrieve clinical notes along with their similarity scores, \n",
    "# allowing for more precise filtering and ranking of results.\n",
    "\n",
    "# Key Components:\n",
    "#   - similarity_search_with_relevance_scores(query, k=10): \n",
    "#     Retrieves the top k (10 in this case) most similar documents along with their relevance scores.\n",
    "#   - vector_store_chunks.similarity_search_with_relevance_scores(): \n",
    "#     Searches within the chunked vector store, returning segment-level matches.\n",
    "#   - vector_store.similarity_search_with_relevance_scores(): \n",
    "#     (Alternative) Searches within the full document vector store for broader context.\n",
    "#   - print(results): Displays the retrieved documents along with their similarity scores.\n",
    "\n",
    "# Score Interpretation:\n",
    "#   - 0.9 - 1.0: Highly relevant match\n",
    "#   - 0.7 - 0.9: Strong relevance\n",
    "#   - 0.5 - 0.7: Moderate relevance\n",
    "#   - 0.3 - 0.5: Low relevance\n",
    "#   - 0.0 - 0.3: Minimal or no relevance\n",
    "\n",
    "# Purpose:\n",
    "# This method provides greater transparency in retrieval by returning similarity scores,\n",
    "# enabling fine-tuned filtering to ensure only highly relevant clinical notes are used for AI analysis.\n",
    "\n",
    "\n",
    "results = vector_store_chunks.similarity_search_with_relevance_scores(query, k=10)\n",
    "# results = vector_store_full.similarity_search_with_relevance_scores(query, k=5)\n",
    "\n",
    "# Print retrieved results with relevance scores in a structured format\n",
    "print(\"\\n=== Retrieved Clinical Notes with Relevance Scores ===\\n\")\n",
    "\n",
    "for idx, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"Document {idx}:\")\n",
    "    print(f\"  Relevance Score: {score:.6f}\")  \n",
    "    print(f\"  Patient Num: {doc.metadata.get('patient_num', 'N/A')}\")\n",
    "    print(f\"  Visit Date: {doc.metadata.get('visit_date', 'N/A')}\")\n",
    "    print(f\"  Document ID: {doc.id}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:500]}...\")  \n",
    "    print(\"-\" * 100)  \n",
    "\n",
    "    "
   ],
   "id": "4220af54d614117b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6.3. Using a Retriever with a Score Threshold\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to configure a retriever to only return documents that meet a minimum relevance score.\n",
    "\n",
    "# Key Components:\n",
    "#   - search_type=\"similarity_score_threshold\": \n",
    "#     Specifies that the retriever should apply a similarity score filter when retrieving documents.\n",
    "#   - search_kwargs={\"k\": 10, \"score_threshold\": score_threshold}: \n",
    "#     - k: Number of top results to return.\n",
    "#     - score_threshold: Minimum relevance score required for a document to be included.\n",
    "#   - retriever.invoke(query): Retrieves documents that meet the score threshold criteria.\n",
    "#   - print(retrieved_docs): Displays the filtered, relevant documents.\n",
    "#   - print(f\"Total relevant results: {len(retrieved_docs)}\"): Outputs the count of retrieved documents.\n",
    "\n",
    "# Purpose:\n",
    "# This method optimizes search precision by ensuring only documents with high relevance scores are retrieved,\n",
    "# making it particularly useful for medical applications requiring accurate and relevant clinical information.\n",
    "\n",
    "\n",
    "retriever = vector_store_chunks.as_retriever(\n",
    "# retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"k\": 10, \n",
    "                   \"score_threshold\": 0.45\n",
    "                   }\n",
    ")\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Print retrieved results with relevance scores in a structured format\n",
    "print(\"\\n=== Retrieved Clinical Notes with Score Threshold ===\\n\")\n",
    "\n",
    "for idx, (doc) in enumerate(results, 1):\n",
    "    print(f\"Document {idx}:\")\n",
    "    print(f\"  Patient Num: {doc.metadata.get('patient_num', 'N/A')}\")\n",
    "    print(f\"  Visit Date: {doc.metadata.get('visit_date', 'N/A')}\")\n",
    "    print(f\"  Document ID: {doc.id}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:500]}...\")  \n",
    "    print(\"-\" * 100)  \n",
    "\n",
    "print(f\"Total relevant results: {len(results)}\")\n"
   ],
   "id": "95aba0fe53c9eb30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 6.4. Performing Maximal Marginal Relevance (MMR) Search\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to retrieve clinical notes using MMR, balancing relevance and diversity.\n",
    "\n",
    "# Key Components:\n",
    "#   - max_marginal_relevance_search(): Retrieves results that maximize relevance while reducing redundancy.\n",
    "#   - fetch_k=50: Specifies the number of documents to fetch before applying MMR selection.\n",
    "#   - k=10: Specifies the number of final documents to return.\n",
    "#   - lambda_mult=0.5: Controls the balance between relevance (1) and diversity (0). \n",
    "#     - 0: Maximizes diversity in results.\n",
    "#     - 1: Prioritizes relevance, potentially leading to similar documents.\n",
    "#   - print(results): Displays the retrieved documents.\n",
    "\n",
    "# Purpose:\n",
    "# MMR ensures that the retrieved clinical notes are not only relevant but also diverse,\n",
    "# reducing redundancy and covering a broader range of information. \n",
    "# This is especially useful in medical applications where multiple perspectives on a condition or treatment are needed.\n",
    "\n",
    "\n",
    "results = vector_store_chunks.max_marginal_relevance_search(\n",
    "# results = vector_store_full.max_marginal_relevance_search(\n",
    "    query, \n",
    "    k=10, \n",
    "    fetch_k=100, \n",
    "    lambda_mult=0.5)\n",
    "\n",
    "# Print retrieved results with relevance scores in a structured format\n",
    "print(\"\\n=== Retrieved Clinical Notes with MMR Search ===\\n\")\n",
    "\n",
    "for idx, (doc) in enumerate(results, 1):\n",
    "    print(f\"Document {idx}:\")\n",
    "    print(f\"  Patient Num: {doc.metadata.get('patient_num', 'N/A')}\")\n",
    "    print(f\"  Visit Date: {doc.metadata.get('visit_date', 'N/A')}\")\n",
    "    print(f\"  Document ID: {doc.id}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:500]}...\")  \n",
    "    print(\"-\" * 100)  \n",
    "\n",
    "print(f\"Total relevant results: {len(results)}\")\n"
   ],
   "id": "a0ad8bee9367f992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Generation\n",
    "\n",
    "**Key Steps:**\n",
    "1. **Creating a Prompt Template for LLM Querying (Item 7.1)**:\n",
    "   - Demonstrates how to structure a prompt for an AI model to analyze clinical notes.\n",
    "2. **Invoking AzureChatOpenAI with Retrieved Context (Item 7.2)**:\n",
    "   - Demonstrates how to pass retrieved clinical notes into the AI model for generating structured responses.\n",
    "\n",
    "<img src=\"./images/rag_generation.png\" alt=\"RAG Retrieval\" width=\"1250\">\n"
   ],
   "id": "9b954e41b6d9ce64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7.1. Creating a Prompt Template for LLM Querying\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to structure a prompt for an AI model to analyze clinical notes.\n",
    "\n",
    "# Key Components:\n",
    "#   - PromptTemplate.from_template(): Creates a dynamic prompt template for AI interaction.\n",
    "#   - {context}: Placeholder for retrieved clinical notes that will provide context for the LLM.\n",
    "#   - {query}: Placeholder for the user’s query, which the AI will answer based on the provided context.\n",
    "#   - Structured Output:\n",
    "#     - Patient Num, Gender, Age, and Race fields ensure that the response is structured and complete.\n",
    "#     - Summary: Ensures that the AI-generated output provides a concise, yet informative response to the query.\n",
    "\n",
    "# Purpose:\n",
    "# This prompt template ensures that the AI model generates structured, clear, and relevant responses\n",
    "# when analyzing clinical notes, making it suitable for automated medical documentation and decision support.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \n",
    "    \"You are a medical assistant analyzing clinical notes. Based on the following records:\\n\\n\"\n",
    "    \n",
    "    \"{retrieved_docs}\\n\\n\"\n",
    "    \n",
    "    \"Answer the question: {query} using the following structure:\\n\"\n",
    "    \"   - Patient Num: patient_num, Gender: , Age: , Race: \"\n",
    "    \"   - Visit Date: visit_date\\n\" \n",
    "    \"   - Summary: One paragraph summarizing the patient note and one paragraph answering the question\"\n",
    "    \n",
    ")\n"
   ],
   "id": "8f86aa297dd40247",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 7.2. Invoking AzureChatOpenAI with Retrieved Context\n",
    "# -----------------------------------------------------------\n",
    "# Demonstrates how to pass retrieved clinical notes into the AI model for generating structured responses.\n",
    "\n",
    "# Key Components:\n",
    "#   - final_prompt = prompt_template.format(context=results, query=query): \n",
    "#     - Populates the prompt template with the retrieved clinical notes (context) and the user query.\n",
    "#   - model.invoke(final_prompt): \n",
    "#     - Sends the structured prompt to the Azure OpenAI model for processing.\n",
    "#   - print(response.content): \n",
    "#     - Displays the AI-generated response.\n",
    "\n",
    "# Purpose:\n",
    "# This step completes the RAG (Retrieval-Augmented Generation) workflow by allowing the LLM to analyze relevant \n",
    "# clinical notes and generate structured, insightful answers to medical queries.\n",
    "\n",
    "# Format the final prompt\n",
    "final_prompt = prompt_template.format(retrieved_docs=results, query=query)\n",
    "\n",
    "# Invoke Azure OpenAI model with the RAG-enhanced prompt\n",
    "response = model.invoke(final_prompt)\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ],
   "id": "ca74b51f89d82fcb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
